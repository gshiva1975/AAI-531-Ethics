# -*- coding: utf-8 -*-
"""lab-assignment_3.1_notebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xOdGm678D4avhE2V0o93zlFdTkuAFjZK

# Bias and Fairness Metrics Lab and Assignment Notebook

In this lab, we will explore the concept of bias in models and datasets, with a focus on identifying and mitigating fairness issue. Bias in machine learning can arise due to imbalances in the data, societal inequities, or algorithmic processing, and it can have significant consequences in real-world applications, such as lending decisions, hiring processes, or healthcare recommendations.

Using IBM's AIF360 re-weighing and the disparate impact remover (DIR) you will investigate how modifying a dataset can improve fairness metrics while assessing its potential trade-offs. We will be using a credit card default dataset where we will:

**Lab Portion:**
1. Quantify dataset bias: by calculating fairness metrics such as Statistical Parity Difference (SPD), Equal Opportunity Difference (EOD), and Disparate Impact (DI).
2. Mitigate Bias: using AIF360's reweighing or DIR.
3. Analyze trade-offs.

*Adapted from: Masis, S. (2021). Chapter 11: Bias Mitigation and Causal Inference Methods. In Interpretable machine learning with Python: Learn to build interpretable high-performance models with hands-on real-world examples. Packt Publishing.

**Assignment Portion:**
1. Experimentation with the DIR
2. Reflection Questions

## Dataset Introduction: Adapted Taiwan Credit Card Default Prediction

The dataset used in this lab is an adapted version of the Taiwan Credit Card Default Prediction Dataset, which contains information on 30,000 credit card clients in Taiwan. It is commonly used in machine learning to study credit risk analysis, as well as fairness and bias in predictive modeling.

The dataset includes a mix of demographic, financial, and behavioral variables, along with a target variable that indicates whether a client defaulted on their credit card payment in the following month. Here's an overview of the key features:

<b>Demographic Variables:</b>
- AGE: The age of the client.
- SEX: The gender of the client (Male: 1, Female: 2).
- EDUCATION: The highest level of education attained (e.g., Graduate, University, High School).
- MARRIAGE: The marital status of the client (e.g., Married, Single, Other).

<b>Financial Variables:</b>
- LIMIT_BAL: The credit limit assigned to the client.
- BILL_AMT1 to BILL_AMT6: The amount of the bill statement for the past six months.
- PAY_AMT1 to PAY_AMT6: The payment amount for the past six months.

<b>Behavioral Variables:</b>
- PAY_0 to PAY_6: The repayment status for the past six months (e.g., fully paid, delayed).

<b>Target Variable:</b>
- default payment next month: Whether the client defaulted on their payment in the next month (1 for default, 0 for no default).
"""

!pip install aif360
!pip install fairlearn

# Import libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score
from aif360.datasets import BinaryLabelDataset
from aif360.metrics import BinaryLabelDatasetMetric
from aif360.algorithms.preprocessing import Reweighing
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, accuracy_score
from aif360.metrics import ClassificationMetric
import lightgbm as lgb
from lightgbm import LGBMClassifier
from tqdm import tqdm
from aif360.algorithms.preprocessing import DisparateImpactRemover
import warnings
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Load the dataset
data = pd.read_csv('/content/drive/My Drive/Colab Notebooks/AAI-531/credit_card_default_531v.1.csv')

# Now you can work with the dataframe 'df'
print(data.head())

# structure and data types
data.info()

# missing values check
missing_values = data.isnull().sum()
print("Missing values:\n", missing_values)

# unique values in columns related to gender <- dataset uses 1 for male and 2 for female
print("Unique values in 'SEX':", data['SEX'].unique())

# Rename only the target column for consistency
data.rename(columns={"default payment next month": "DEFAULT_NEXT_MONTH"}, inplace=True)

# encode SEX column: Male = 1, Female = 0 (already numeric, we are just replace 2 with 0)
data["SEX"] = data["SEX"].replace({2: 0})  # Male = 1, Female = 0

#EDUCATION: Map numbers to clear numeric categories for interpretability
#Graduate = 1, University = 2, High School = 3, Other = 4
data["EDUCATION"] = data["EDUCATION"].replace({
    0: 4,  # Map 0 to 'Other'
    1: 1,  # Graduate
    2: 2,  # University
    3: 3,  # High School
    4: 4,  # Other
    5: 4,  # Other
    6: 4   # Other
})

#MARRIAGE: Map numbers to clear numeric categories for interpretability
#Married = 1, Single = 2, Other = 3
data["MARRIAGE"] = data["MARRIAGE"].replace({
    0: 3,  # Map 0 to 'Other'
    1: 1,  # Married
    2: 2,  # Single
    3: 3   # Other
})

columns_to_keep = ["LIMIT_BAL", "SEX", "EDUCATION", "MARRIAGE", "AGE", "DEFAULT_NEXT_MONTH"]
data_subset = data[columns_to_keep]

"""## Data Exploration"""

# mean default rate by age
age_default_prob = data.groupby("AGE")["DEFAULT_NEXT_MONTH"].mean()

# plotting the probability of default by age
plt.figure(figsize=(12, 6))
sns.lineplot(x=age_default_prob.index, y=age_default_prob.values, marker="o", linestyle="-", color="blue")
plt.title("Probability of Default by Age", fontsize=16)
plt.xlabel("Age", fontsize=14)
plt.ylabel("Probability of Default", fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.grid(axis="both", linestyle="--", alpha=0.7)
plt.tight_layout()
plt.show()

# bin ages into deciles
age_bins = pd.qcut(data["AGE"], q=10, duplicates="drop")
data["Age_Group"] = age_bins

# get range labels for the deciles
age_range_labels = [
    f"{int(interval.left)}-{int(interval.right)}"
    for interval in age_bins.cat.categories
]

# mapping these range labels back to the data
data["Age_Group"] = data["Age_Group"].cat.rename_categories(age_range_labels)

# calculate default probabilities for each age group
age_default_prob = (
    data.groupby("Age_Group", observed=True)["DEFAULT_NEXT_MONTH"]
    .mean()
    .reset_index()
    .rename(columns={"DEFAULT_NEXT_MONTH": "Default_Probability"})
)

# plotting default probabilities
plt.figure(figsize=(14, 7))
sns.barplot(
    data=age_default_prob,
    x="Age_Group",
    y="Default_Probability",
    palette="coolwarm",
    edgecolor="black",
)
plt.axhline(
    y=age_default_prob["Default_Probability"].mean(),
    color="red",
    linestyle="--",
    linewidth=1.5,
    label="Mean Default Probability",
)
plt.title("Probability of Default by Age Group (Deciles)", fontsize=16, fontweight="bold")
plt.xlabel("Age Range (Deciles)", fontsize=14)
plt.ylabel("Probability of Default", fontsize=14)
plt.xticks(rotation=45, fontsize=12)
plt.yticks(fontsize=12)
plt.legend(fontsize=12, loc="upper right")
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.tight_layout()
plt.show()

# define privileged (26-47) and underprivileged groups (21-25, 48+)
data["Age_Group"] = data["AGE"].apply(
    lambda x: "Privileged (Ages 26-47)" if 26 <= x <= 47 else "Underprivileged (Ages 21-25, 48+)"
)

# calculate default probabilities for each group
age_group_default_prob = (
    data.groupby("Age_Group", observed=True)["DEFAULT_NEXT_MONTH"]
    .mean()
    .reset_index()
    .rename(columns={"DEFAULT_NEXT_MONTH": "Default_Probability"})
)

# plot default probabilities with a regression line
plt.figure(figsize=(12, 7))
sns.barplot(
    data=age_group_default_prob,
    x="Age_Group",
    y="Default_Probability",
    palette="coolwarm",
    edgecolor="black"
)
sns.regplot(
    x=np.arange(len(age_group_default_prob)),
    y=age_group_default_prob["Default_Probability"],
    scatter=False, color="blue", label="Regression Line", ci=None
)
plt.axhline(
    y=age_group_default_prob["Default_Probability"].mean(),
    color="red",
    linestyle="--",
    linewidth=1.5,
    label="Mean Default Probability"
)
plt.title("Probability of Default by Age Group", fontsize=16, fontweight="bold")
plt.xlabel("Age Group", fontsize=14)
plt.ylabel("Probability of Default", fontsize=14)
plt.xticks(ticks=np.arange(len(age_group_default_prob)), labels=age_group_default_prob["Age_Group"], fontsize=12)
plt.yticks(fontsize=12)
plt.legend(fontsize=12, loc="upper right")
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.tight_layout()
plt.show()

# define Age_Group for privileged and underprivileged groups
youngest_quartile = data["AGE"].quantile(0.25)
oldest_quartile = data["AGE"].quantile(0.75)

data["Age_Group"] = data["AGE"].apply(
    lambda x: 1 if youngest_quartile < x <= oldest_quartile else 0
)  # 1 for Privileged 0 for Underprivileged

# dbouble checking the gender encoding
data["Gender_Label"] = data["SEX"].replace({1: "Male", 0: "Female"})

# underprivileged group
underprivileged_group = data[data["Age_Group"] == 0]

# default probabilities for males and females in the underprivileged group
default_rates_underprivileged = (
    underprivileged_group.groupby("Gender_Label")["DEFAULT_NEXT_MONTH"].mean() * 100
)

# bar plot for default rates in the underprivileged group x gender
plt.figure(figsize=(10, 6))
sns.barplot(
    x=default_rates_underprivileged.index,
    y=default_rates_underprivileged.values,
    palette=["orange", "green"],
    edgecolor="black"
)
plt.title("Default Rates for Underprivileged Group by Gender", fontsize=16, fontweight="bold")
plt.ylabel("Default Rate (%)", fontsize=14)
plt.xlabel("Gender", fontsize=14)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.tight_layout()
plt.show()

"""## Quantifying Dataset Bias"""

# set seed
rand = 531
np.random.seed(rand)

# defining our target and features
y = data["DEFAULT_NEXT_MONTH"]
X = data.drop(["DEFAULT_NEXT_MONTH"], axis=1).copy()

# splitting into training and testing datasets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=rand)

# privileged and underprivileged Groups for Age
X_train["Age_Group"] = X_train["AGE"].apply(lambda x: 1 if 26 <= x <= 47 else 0).copy()
X_test["Age_Group"] = X_test["AGE"].apply(lambda x: 1 if 26 <= x <= 47 else 0).copy()

# gender
X_train["Gender_Label"] = X_train["SEX"].replace({1: 1, 0: 0})  # female 0, male 1
X_test["Gender_Label"] = X_test["SEX"].replace({1: 1, 0: 0})

# combine X_train and y_train for BinaryLabelDataset
train_ds = BinaryLabelDataset(
    df=X_train.join(y_train),
    label_names=["DEFAULT_NEXT_MONTH"],
    protected_attribute_names=["Age_Group", "Gender_Label"],
    favorable_label=0,  # 0 did not default
    unfavorable_label=1  # 1 defaulted
)

# defining our groups, remember- privileged is 26-47 and underprivileged is 21-25, & 48+
privileged_groups = [{"Age_Group": 1}]
unprivileged_groups = [{"Age_Group": 0}]

# then, compute our metrics for training data
metrics_train_ds = BinaryLabelDatasetMetric(
    train_ds,
    unprivileged_groups=unprivileged_groups,
    privileged_groups=privileged_groups
)

# print results from training data
print("Training Data Metrics:")
print(f"Statistical Parity Difference (SPD): {metrics_train_ds.statistical_parity_difference():.4f}")
print(f"Disparate Impact (DI): {metrics_train_ds.disparate_impact():.4f}")
print(f"Smoothed Empirical Differential Fairness (SEDF): {metrics_train_ds.smoothed_empirical_differential_fairness():.4f}")

"""## Quantifying Model Bias"""

# train our LightGBM classifier
lgb_params = {
    'learning_rate': 0.4,
    'reg_alpha': 21,
    'reg_lambda': 1,
    'scale_pos_weight': 1.8
}

lgb_base_model = LGBMClassifier(random_seed=531, max_depth=6, num_leaves=33, **lgb_params)
lgb_base_model.fit(X_train, y_train)

# evaluate model
y_pred_test = lgb_base_model.predict(X_test)
y_pred_prob_test = lgb_base_model.predict_proba(X_test)[:, 1]

# performance metrics
accuracy = accuracy_score(y_test, y_pred_test)
precision = precision_score(y_test, y_pred_test)
recall = recall_score(y_test, y_pred_test)
f1 = f1_score(y_test, y_pred_test)
roc_auc = roc_auc_score(y_test, y_pred_prob_test)

print(f"Accuracy (Test): {accuracy:.4f}")
print(f"Precision (Test): {precision:.4f}")
print(f"Recall (Test): {recall:.4f}")
print(f"F1-Score (Test): {f1:.4f}")
print(f"ROC-AUC (Test): {roc_auc:.4f}")

"""<b>Training Metrics:</b>
- Accuracy (Train): 0.8406
- Precision (Train): 0.6911
- Recall (Train): 0.6013
- F1-Score (Train): 0.6431
- ROC-AUC (Train): 0.8802

<b>Test Metrics:</b>
- Accuracy (Test): 0.8176
- Precision (Test): 0.6488
- Recall (Test): 0.5404
- F1-Score (Test): 0.5897
- ROC-AUC (Test): 0.79917991.8802
"""

# there was a biggest decrease in the ROC curve, so it is worthwhile to visualize it
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_test)
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, label=f"ROC Curve (AUC = {roc_auc:.4f})")
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")
plt.title("ROC Curve")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.legend()
plt.grid(True)
plt.show()

# passing test_ds into BinaryLabelDataset
test_ds = BinaryLabelDataset(
    df=X_test.join(y_test),
    label_names=["DEFAULT_NEXT_MONTH"],
    protected_attribute_names=["Age_Group", "Gender_Label"],
    favorable_label=0,  # 0 no default
    unfavorable_label=1  # 1 defaulted
)
# add predictions and scores to the test dataset for our fairness metrics
test_pred_ds = test_ds.copy(deepcopy=True)
test_pred_ds.labels = y_pred_test.reshape(-1, 1)
test_pred_ds.scores = y_pred_prob_test.reshape(-1, 1)

# Compute fairness metrics
metrics_test_cls = ClassificationMetric(
    test_ds,
    test_pred_ds,
    unprivileged_groups=unprivileged_groups,
    privileged_groups=privileged_groups
)

# print fairness metrics
print("\nFairness Metrics on Test Data:")
print(f"Statistical Parity Difference (SPD): {metrics_test_cls.statistical_parity_difference():.4f}")
print(f"Disparate Impact (DI): {metrics_test_cls.disparate_impact():.4f}")
print(f"Smoothed Empirical Differential Fairness (SEDF): {metrics_train_ds.smoothed_empirical_differential_fairness():.4f}")

"""<b>Training Data Metrics:</b>
- Statistical Parity Difference (SPD): -0.1202
- Disparate Impact (DI): 0.8480
- Smoothed Empirical Differential Fairness (SEDF): 0.8774

## Mitigating Bias

### Pre-Processing Bias Mitigation Methods

#### Reweighting (Reweighing) Method
"""

# apply reweighing
reweighter = Reweighing(
    unprivileged_groups=unprivileged_groups,
    privileged_groups=privileged_groups
)
reweighter.fit(train_ds)
train_rw_ds = reweighter.transform(train_ds)

# metrics for the reweighted training dataset
metrics_train_rw_ds = BinaryLabelDatasetMetric(
    train_rw_ds,
    unprivileged_groups=unprivileged_groups,
    privileged_groups=privileged_groups
)

# print metrics
print('Reweighted Training Data Metrics:')
print(f'Statistical Parity Difference (SPD): {metrics_train_rw_ds.statistical_parity_difference():.4f}')
print(f'Disparate Impact (DI): {metrics_train_rw_ds.disparate_impact():.4f}')
print(f'Smoothed Empirical Differential Fairness (SEDF): {metrics_train_rw_ds.smoothed_empirical_differential_fairness():.4f}')

"""### Training LightGBM on the Reweighted Dataset"""

# train LightGbM
lgb_rw_model = LGBMClassifier(
    random_state=531,
    max_depth=6,
    num_leaves=33,
    **lgb_params
)

# use reweighted dataset's instance weights
lgb_rw_model.fit(
    X_train,
    y_train,
    sample_weight=train_rw_ds.instance_weights
)

# evaluate performance metrics
y_pred_test_rw = lgb_rw_model.predict(X_test)
y_pred_prob_test_rw = lgb_rw_model.predict_proba(X_test)[:, 1]

# Compute performance metrics
accuracy_rw = accuracy_score(y_test, y_pred_test_rw)
precision_rw = precision_score(y_test, y_pred_test_rw)
recall_rw = recall_score(y_test, y_pred_test_rw)
f1_rw = f1_score(y_test, y_pred_test_rw)
roc_auc_rw = roc_auc_score(y_test, y_pred_prob_test_rw)

# display
print("\nPerformance Metrics for Reweighted Model:")
print(f"Accuracy (Test): {accuracy_rw:.4f}")
print(f"Precision (Test): {precision_rw:.4f}")
print(f"Recall (Test): {recall_rw:.4f}")
print(f"F1-Score (Test): {f1_rw:.4f}")
print(f"ROC-AUC (Test): {roc_auc_rw:.4f}")

# compute fairness metrics & add predictions/scores to test ds
test_pred_rw_ds = test_ds.copy(deepcopy=True)
test_pred_rw_ds.labels = y_pred_test_rw.reshape(-1, 1)
test_pred_rw_ds.scores = y_pred_prob_test_rw.reshape(-1, 1)

# fairness metrics for the reweighted model
metrics_test_rw_cls = ClassificationMetric(
    test_ds,
    test_pred_rw_ds,
    unprivileged_groups=unprivileged_groups,
    privileged_groups=privileged_groups
)

# print
print("\nFairness Metrics for Reweighted Model on Test Data:")
print(f"Statistical Parity Difference (SPD): {metrics_test_rw_cls.statistical_parity_difference():.4f}")
print(f"Disparate Impact (DI): {metrics_test_rw_cls.disparate_impact():.4f}")
print(f"Equal Opportunity Difference (EOD): {metrics_test_rw_cls.equal_opportunity_difference():.4f}")
print(f"Average Odds Difference (AOD): {metrics_test_rw_cls.average_odds_difference():.4f}")
print(f"Differential Fairness Bias Amplification (DFBA): {metrics_test_rw_cls.differential_fairness_bias_amplification():.4f}")

# comparing results
print("\nComparison of Original and Reweighted Models:")
print("\nPerformance Metrics:")
print(f"Original Model Accuracy: {accuracy:.4f} | Reweighted Model Accuracy: {accuracy_rw:.4f}")
print(f"Original Model Precision: {precision:.4f} | Reweighted Model Precision: {precision_rw:.4f}")
print(f"Original Model Recall: {recall:.4f} | Reweighted Model Recall: {recall_rw:.4f}")
print(f"Original Model F1-Score: {f1:.4f} | Reweighted Model F1-Score: {f1_rw:.4f}")
print(f"Original Model ROC-AUC: {roc_auc:.4f} | Reweighted Model ROC-AUC: {roc_auc_rw:.4f}")

print("\nFairness Metrics:")
print(f"Original SPD: {metrics_test_cls.statistical_parity_difference():.4f} | Reweighted SPD: {metrics_test_rw_cls.statistical_parity_difference():.4f}")
print(f"Original DI: {metrics_test_cls.disparate_impact():.4f} | Reweighted DI: {metrics_test_rw_cls.disparate_impact():.4f}")
print(f"Original SEDF: {metrics_test_cls.smoothed_empirical_differential_fairness():.4f} | Reweighted SEDF: {metrics_train_rw_ds.smoothed_empirical_differential_fairness():.4f}")

"""## Disparate Impact Remover (DIR)

The main goal of the DIR is to remove or reduce the disparate impact (unfair treatment) that arises due to sensitive or protected attributes (i.e. age, gender). This is done by "repairing" the data so that the values of the sensitive attribute(s) have minimal influence on the non-sensitive attributes while maintaining the relationship with the target variable.

The repair level ranges from 0 to 1.
- Repair Level = 0: No modification to the dataset, the data remains unchanged.
- Repair Level = 1: The sensitive attribute is completely removed, and the features are "repaired" to have no correlation with the sensitive attribute.
- Intermediate Levels (i.e. 0.5) balance between preserving data utility and reducing disparate impact.

<b>Benefits:</b>Removes bias before training the model, so it can be applied before any ML model and ensures that modified features are less correlated with sensitive attributes, leading to fairer predictions.

<b>Limitations:</b>Reducing the influence of sensitive attributes might reduce the predictive performance of the model. Also, DIR requires a balance in the repair level to avoid overcorrection which could potentially remove useful information.
"""

#!pip install aif360
!pip install BlackBoxAuditing
!pip install --upgrade aif360 lightgbm

# define the levels for repair
levels = np.hstack([np.linspace(0., 0.1, 41), np.linspace(0.2, 1, 9)])
protected_index = train_ds.feature_names.index("Age_Group")

# initialize variables to track the best repair level
di = np.array([])  # Collect Disparate Impact for all levels
train_dir_ds = None
test_dir_ds = None
X_train_dir = None
X_test_dir = None
lgb_dir_model = None

# loop through repair levels
for level in tqdm(levels):
    #apply DIR at the current repair level
    di_remover = DisparateImpactRemover(repair_level=level)
    train_dir_ds_i = di_remover.fit_transform(train_ds)
    test_dir_ds_i = di_remover.fit_transform(test_ds)

    #remove protected attribute from the features
    X_train_dir_i = np.delete(train_dir_ds_i.features, protected_index, axis=1)
    X_test_dir_i = np.delete(test_dir_ds_i.features, protected_index, axis=1)

    # train LightGBM model on the repaired dataset
    lgb_dir_model_i = lgb.LGBMClassifier(
        random_state=rand, max_depth=5, num_leaves=33, verbose=-1, **lgb_params
    )
    lgb_dir_model_i.fit(X_train_dir_i, train_dir_ds_i.labels)

    #predict on the repaired test dataset
    test_dir_ds_pred_i = test_dir_ds_i.copy()
    test_dir_ds_pred_i.labels = lgb_dir_model_i.predict(X_test_dir_i)

    # fairness metrics
    metrics_test_dir_ds = BinaryLabelDatasetMetric(
        test_dir_ds_pred_i,
        unprivileged_groups=unprivileged_groups,
        privileged_groups=privileged_groups,
    )
    di_i = metrics_test_dir_ds.disparate_impact()

    # track and print DI for this level
    print(f"Repair Level: {level:.2f}, Disparate Impact: {di_i:.4f}")

    # update the best results if this level is closest to DI=1
    if (di.shape[0] == 0) or (np.min(np.abs(di - 1)) >= abs(di_i - 1)):
        train_dir_ds = train_dir_ds_i
        test_dir_ds = test_dir_ds_i
        X_train_dir = X_train_dir_i
        X_test_dir = X_test_dir_i
        lgb_dir_model = lgb_dir_model_i

    # append to DI list
    di = np.append(di, di_i)

# Plot Disparate Impact across repair levels
import matplotlib.pyplot as plt
import seaborn as sns

di = di[:len(levels)]

# Use Seaborn style for better visuals
sns.set(style="whitegrid")

plt.figure(figsize=(11, 6))

# Plot the Disparate Impact (DI) against repair levels
plt.plot(levels, di, marker="o", linestyle="-", color="blue", label="Disparate Impact")

# Add labels, title, and legend
plt.ylabel("Disparate Impact (DI)", fontsize=14)
plt.xlabel("Repair Level", fontsize=14)
plt.title("Disparate Impact Across Repair Levels", fontsize=16)
plt.legend(fontsize=12)

# Show grid for readability
plt.grid(True)

# Display the plot
plt.show()

from IPython.display import display
display(plt.gcf())

# print the best repair level and its corresponding disparate impact
best_level = levels[np.argmin(np.abs(di - 1))]
print(f"Best Repair Level: {best_level:.2f}")
print(f"Disparate Impact at Best Level: {di[np.argmin(np.abs(di - 1))]:.4f}")

# predict on the modified test data (from DIR)
y_pred_test_dir = lgb_dir_model.predict(X_test_dir)
y_pred_prob_test_dir = lgb_dir_model.predict_proba(X_test_dir)[:, 1]

# compute performance metrics on the test data
accuracy_test_dir = accuracy_score(y_test, y_pred_test_dir)
precision_test_dir = precision_score(y_test, y_pred_test_dir)
recall_test_dir = recall_score(y_test, y_pred_test_dir)
f1_test_dir = f1_score(y_test, y_pred_test_dir)
roc_auc_test_dir = roc_auc_score(y_test, y_pred_prob_test_dir)

#display
print("Performance Metrics on Test Data (DIR Applied):")
print(f"Accuracy: {accuracy_test_dir:.4f}")
print(f"Precision: {precision_test_dir:.4f}")
print(f"Recall: {recall_test_dir:.4f}")
print(f"F1-Score: {f1_test_dir:.4f}")
print(f"ROC-AUC: {roc_auc_test_dir:.4f}")

# predictions and scores to the BinaryLabelDataset for our fairness metrics
test_dir_ds_pred = test_dir_ds.copy(deepcopy=True)
test_dir_ds_pred.labels = y_pred_test_dir.reshape(-1, 1)
test_dir_ds_pred.scores = y_pred_prob_test_dir.reshape(-1, 1)

# fairness metrics
metrics_test_dir_cls = ClassificationMetric(
    test_dir_ds,
    test_dir_ds_pred,
    unprivileged_groups=unprivileged_groups,
    privileged_groups=privileged_groups
)

# display
print("\nFairness Metrics on Test Data (DIR Applied):")
print(f"Statistical Parity Difference (SPD): {metrics_test_dir_cls.statistical_parity_difference():.4f}")
print(f"Disparate Impact (DI): {metrics_test_dir_cls.disparate_impact():.4f}")
print(f"Equal Opportunity Difference (EOD): {metrics_test_dir_cls.equal_opportunity_difference():.4f}")
print(f"Average Odds Difference (AOD): {metrics_test_dir_cls.average_odds_difference():.4f}")
print(f"Differential Fairness Bias Amplification (DFBA): {metrics_test_dir_cls.differential_fairness_bias_amplification():.4f}")

# Comparison of ALL Performance Metrics
print("\nComparison of Performance Metrics Across Models:")
print(f"{'Metric':<15}{'Original':<15}{'Reweighted':<15}{'DIR Applied':<15}")
print("-" * 60)
print(f"{'Accuracy':<15}{accuracy:<15.4f}{accuracy_rw:<15.4f}{accuracy_test_dir:<15.4f}")
print(f"{'Precision':<15}{precision:<15.4f}{precision_rw:<15.4f}{precision_test_dir:<15.4f}")
print(f"{'Recall':<15}{recall:<15.4f}{recall_rw:<15.4f}{recall_test_dir:<15.4f}")
print(f"{'F1-Score':<15}{f1:<15.4f}{f1_rw:<15.4f}{f1_test_dir:<15.4f}")
print(f"{'ROC-AUC':<15}{roc_auc:<15.4f}{roc_auc_rw:<15.4f}{roc_auc_test_dir:<15.4f}")

# Comparison of ALL Fairness Metrics (SPD, DI, SEDF)
print("\nComparison of Fairness Metrics Across Models:")
print(f"{'Metric':<15}{'Original':<15}{'Reweighted':<15}{'DIR Applied':<15}")
print("-" * 60)
print(f"{'SPD':<15}{metrics_test_cls.statistical_parity_difference():<15.4f}{metrics_test_rw_cls.statistical_parity_difference():<15.4f}{metrics_test_dir_cls.statistical_parity_difference():<15.4f}")
print(f"{'DI':<15}{metrics_test_cls.disparate_impact():<15.4f}{metrics_test_rw_cls.disparate_impact():<15.4f}{metrics_test_dir_cls.disparate_impact():<15.4f}")
print(f"{'SEDF':<15}{metrics_test_cls.smoothed_empirical_differential_fairness():<15.4f}{metrics_train_rw_ds.smoothed_empirical_differential_fairness():<15.4f}{metrics_test_dir_cls.smoothed_empirical_differential_fairness():<15.4f}")

"""*****

# Assignment Instructions

<b>Objective:</b>

In this assignment, you will explore the impact of fairness metrics after applying the Disparate Impact Remover (DIR), retrain a Light GBM model with the adjusted data, and compare its metrics (listed below) against the original and reweighted models from the lab tutorial. Afterward, respond to the reflection questions.

<b>Part 1: Experimentation with the DIR (assignment output should be included in 3.1 Exercise Document</b>

-	(Part 1.1a. & 1.1b.): Modify the levels array for the Disparate Impact Remover to include finer and coarser granularity. (See example in lab/assignment notebook.)
-	(Part 1.2): Identify the best repair level and the Disparate Impact at the best repair level.
-	(Part 1.3): Retrain a Light GBM model on the modified DIR data with the best repair level and compute both the performance metrics and the fairness metrics.
"""

from tqdm import tqdm
from aif360.algorithms.preprocessing import DisparateImpactRemover
import lightgbm as lgb
from aif360.metrics import BinaryLabelDatasetMetric
import numpy as np
import warnings
warnings.filterwarnings('ignore')

# Define levels with finer & coarser granularity
levels = np.hstack([np.linspace(0., 0.1, 81), np.linspace(0.2, 1, 20)])


# Generate the levels
granular_levels = np.linspace(0., 0.1, 81)  # Fine granularity (small steps)
coarse_levels = np.linspace(0.2, 1, 20)     # Coarse granularity (larger steps)

# Combine both if needed
levels = np.hstack([granular_levels, coarse_levels])

# Print to verify
print("Granular Levels:", granular_levels)
print("\nCoarse Levels:", coarse_levels)

# Initialize variables
di = np.array([])  # Store Disparate Impact values
best_level = None
protected_index = train_ds.feature_names.index("Age_Group")

LGBMClassifier(force_row_wise=True)

for level in tqdm(levels):
    # Apply Disparate Impact Remover (DIR) at the current repair level
    di_remover = DisparateImpactRemover(repair_level=level)


    train_dir_ds_i = di_remover.fit_transform(train_ds)
    test_dir_ds_i = di_remover.fit_transform(test_ds)

    # Remove the protected attribute (Age_Group)
    X_train_dir_i = np.delete(train_dir_ds_i.features, protected_index, axis=1)
    X_test_dir_i = np.delete(test_dir_ds_i.features, protected_index, axis=1)

    # Train LightGBM model on the repaired dataset
    #lgb_dir_model_i = lgb.LGBMClassifier(random_state=531, max_depth=5, num_leaves=33)
    #lgb_dir_model_i=LGBMClassifier(random_state=10, max_depth=3,force_row_wise=True, num_leaves=33)
    #lgb_model = lgb.LGBMClassifier(
    #max_depth=6,                 # Adjust depth
    #min_data_in_leaf=50,         # Ensure minimum samples per leaf
    #feature_fraction=0.8,        # Use 80% of features per iteration
    #force_row_wise=True          # Reduce processing overhead
    #)

    # train LightGBM model on the repaired dataset
    lgb_dir_model_i = lgb.LGBMClassifier(
        random_state=rand, max_depth=5, num_leaves=33,force_row_wise=True, verbose=-1, **lgb_params
    )
    lgb_dir_model_i.fit(X_train_dir_i, train_dir_ds_i.labels)

    # Predict on the repaired test dataset
    test_dir_ds_pred_i = test_dir_ds_i.copy()
    test_dir_ds_pred_i.labels = lgb_dir_model_i.predict(X_test_dir_i)

    # Compute fairness metrics
    metrics_test_dir_ds = BinaryLabelDatasetMetric(
        test_dir_ds_pred_i,
        unprivileged_groups=unprivileged_groups,
        privileged_groups=privileged_groups,
    )
    di_i = metrics_test_dir_ds.disparate_impact()

    # Track DI for this level
    print(f"Repair Level: {level:.4f}, Disparate Impact: {di_i:.4f}")
    di = np.append(di, di_i)

di_i = di

for level, di_value in tqdm(zip(levels, di)):  # Iterate through levels and corresponding DI values
    print(f"Repair Level: {level:.4f}, Disparate Impact: {di_value:.4f}")

# Find best repair level (DI closest to 1.0)
best_level = levels[np.argmin(np.abs(di - 1))]
print(f"\nBest Repair Level: {best_level:.4f}")
print(f"Disparate Impact at Best Level: {di[np.argmin(np.abs(di - 1))]:.4f}")

# Plot Disparate Impact across repair levels
import matplotlib.pyplot as plt
import seaborn as sns

di_i = di_i[:len(levels)]
#di = di[:len(levels)]

# Use Seaborn style for better visuals
sns.set(style="whitegrid")

plt.figure(figsize=(11, 6))

# Plot the Disparate Impact (DI) against repair levels
plt.plot(levels, di_i, marker="o", linestyle="-", color="pink", label="Disparate Impact")

# Add labels, title, and legend
plt.ylabel("Disparate Impact (DI)", fontsize=14)
plt.xlabel("Repair Level", fontsize=14)
plt.title("Disparate Impact Across Repair Levels", fontsize=16)
plt.legend(fontsize=12)

# Show grid for readability
plt.grid(True)

# Display the plot
plt.show()

from IPython.display import display
display(plt.gcf())

# Generate the levels
granular_levels = np.linspace(0., 0.1, 81)  # Fine granularity (small steps)
coarse_levels = np.linspace(0.2, 1, 20)     # Coarse granularity (larger steps)

# Combine both if needed
levels = np.hstack([granular_levels, coarse_levels])

# Print to verify
print("Granular Levels:", granular_levels)
print("\nCoarse Levels:", coarse_levels)

di_i = di_i[:len(levels)]

# print granular_di = di[:len(granular_levels)] for each level

granular_di = di[:len(granular_levels)]
for level, di_value in zip(granular_levels, granular_di):
    print(f"Repair Level: {level:.4f}, Disparate Impact: {di_value:.4f}")

#  Granular and Coarse level seperately

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns



# Separate granular and coarse levels and their corresponding DI values
granular_levels = np.linspace(0., 0.1, 81)
coarse_levels = np.linspace(0.2, 1, 20)

granular_di = di[:len(granular_levels)]
coarse_di = di[len(granular_levels):]




# Use Seaborn style for better visuals
sns.set(style="whitegrid")

plt.figure(figsize=(11, 6))

# Plot the Disparate Impact (DI) against repair levels
plt.plot(granular_levels, granular_di, marker="o", linestyle="-", color="green", label="Granular Disparate Impact")

# Add labels, title, and legend
plt.ylabel("Disparate Impact (DI)", fontsize=14)
plt.xlabel("Repair Level", fontsize=14)
plt.title("Disparate Impact Across Repair Levels", fontsize=16)
plt.legend(fontsize=12)

# Show grid for readability
plt.grid(True)

# Display the plot
plt.show()

from IPython.display import display
display(plt.gcf())

for level, di_value in zip(coarse_levels, coarse_di):
    print(f"Repair Level: {level:.4f}, Disparate Impact: {di_value:.4f}")

# Use Seaborn style for better visuals
sns.set(style="whitegrid")

plt.figure(figsize=(11, 6))

# Plot the Disparate Impact (DI) against repair levels
plt.plot(coarse_levels, coarse_di, marker="o", linestyle="-", color="green", label="Coarse Disparate Impact")

# Add labels, title, and legend
plt.ylabel("Disparate Impact (DI)", fontsize=14)
plt.xlabel("Repair Level", fontsize=14)
plt.title("Disparate Impact Across Repair Levels", fontsize=16)
plt.legend(fontsize=12)

# Show grid for readability
plt.grid(True)

# Display the plot
plt.show()

from IPython.display import display
display(plt.gcf())

# get the best repair level where Disparate Impact (DI) is closest to 1.0.

import numpy as np
# Find the index of the best repair level where DI is closest to 1
best_level_index = np.argmin(np.abs(di_i - 1))

# Get the best repair level
best_level = levels[best_level_index]

print(f"Best Repair Level: {best_level:.4f}")
print(f"Disparate Impact at Best Level: {di[best_level_index]:.4f}")

# Disparate Impact at the best repair level.

import numpy as np

import numpy as np

def find_best_repair_level(levels, di_values):
    """
    Identifies the best repair level where Disparate Impact (DI) is closest to 1.0.

    Args:
        levels (numpy array): Array of repair levels.
        di_values (numpy array): Array of Disparate Impact values.

    Returns:
        best_repair_level (float): The repair level closest to DI = 1.0.
        best_disparate_impact (float): The Disparate Impact value at the best repair level.
    """
    best_index = np.argmin(np.abs(di_values - 1))  # Find index where DI is closest to 1
    best_repair_level = levels[best_index]
    best_disparate_impact = di_values[best_index]

    return best_repair_level, best_disparate_impact


# Identify the best repair level
best_repair_level, best_disparate_impact = find_best_repair_level(levels, di_i)

# Print results
print(f"Best Repair Level: {best_repair_level:.4f}")
print(f"Disparate Impact at Best Repair Level: {best_disparate_impact:.4f}")

print(f"Disparate Impact at Best Level: {di[np.argmin(np.abs(di - 1))]:.4f}")

from aif360.algorithms.preprocessing import DisparateImpactRemover
from aif360.metrics import ClassificationMetric
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import lightgbm as lgb

# Apply Disparate Impact Remover (DIR) at the best repair level
best_dir = DisparateImpactRemover(repair_level=best_repair_level)
train_dir_ds_best = best_dir.fit_transform(train_ds)
test_dir_ds_best = best_dir.fit_transform(test_ds)

# Remove protected attribute from the features
protected_index = train_ds.feature_names.index("Age_Group")
X_train_dir_best = np.delete(train_dir_ds_best.features, protected_index, axis=1)
X_test_dir_best = np.delete(test_dir_ds_best.features, protected_index, axis=1)

# Train a LightGBM model on the repaired dataset
#lgb_dir_model_best = lgb.LGBMClassifier(random_state=531, max_depth=5, num_leaves=33)
lgb_dir_model_best = lgb.LGBMClassifier(
        random_state=rand, max_depth=5, num_leaves=33,force_row_wise=True, verbose=-1, **lgb_params
    )

lgb_dir_model_best.fit(X_train_dir_best, train_dir_ds_best.labels.ravel())

# Predict on the modified test dataset
y_pred_test_dir_best = lgb_dir_model_best.predict(X_test_dir_best)
y_pred_prob_test_dir_best = lgb_dir_model_best.predict_proba(X_test_dir_best)[:, 1]

# Compute Performance Metrics
accuracy_dir = accuracy_score(y_test, y_pred_test_dir_best)
precision_dir = precision_score(y_test, y_pred_test_dir_best)
recall_dir = recall_score(y_test, y_pred_test_dir_best)
f1_dir = f1_score(y_test, y_pred_test_dir_best)
roc_auc_dir = roc_auc_score(y_test, y_pred_prob_test_dir_best)

# Add predictions & scores to BinaryLabelDataset for Fairness Metrics
test_dir_ds_pred_best = test_dir_ds_best.copy()
test_dir_ds_pred_best.labels = y_pred_test_dir_best.reshape(-1, 1)
test_dir_ds_pred_best.scores = y_pred_prob_test_dir_best.reshape(-1, 1)

# Compute Fairness Metrics
metrics_test_dir_cls_best = ClassificationMetric(
    test_dir_ds_best,
    test_dir_ds_pred_best,
    unprivileged_groups=unprivileged_groups,
    privileged_groups=privileged_groups
)

# Display Performance Metrics
print("Performance Metrics on Test Data (Best DIR Applied):")
print(f"Accuracy: {accuracy_dir:.4f}")
print(f"Precision: {precision_dir:.4f}")
print(f"Recall: {recall_dir:.4f}")
print(f"F1-Score: {f1_dir:.4f}")
print(f"ROC-AUC: {roc_auc_dir:.4f}")

# Display Fairness Metrics
print("\nFairness Metrics on Test Data (Best DIR Applied):")
print(f"Statistical Parity Difference (SPD): {metrics_test_dir_cls_best.statistical_parity_difference():.4f}")
print(f"Disparate Impact (DI): {metrics_test_dir_cls_best.disparate_impact():.4f}")
print(f"Equal Opportunity Difference (EOD): {metrics_test_dir_cls_best.equal_opportunity_difference():.4f}")
print(f"Average Odds Difference (AOD): {metrics_test_dir_cls_best.average_odds_difference():.4f}")
print(f"Differential Fairness Bias Amplification (DFBA): {metrics_test_dir_cls_best.differential_fairness_bias_amplification():.4f}")

"""**Part 2: Reflection Questions (to be answered in excercise document)**

2.1. How does the Disparate Impact (DI) change with finer granularity? Coarser granularity?
"""

#  How does the Disparate Impact (DI) change with finer granularity and Coarser granularity? use
# # Separate granular and coarse levels and their corresponding DI values


import matplotlib.pyplot as plt
import numpy as np
# Assuming 'levels', 'di' (from the original code) are already defined

# Separate granular and coarse levels and their corresponding DI values
granular_levels = np.linspace(0., 0.1, 81)
coarse_levels = np.linspace(0.2, 1, 20)

granular_di = di[:len(granular_levels)]
coarse_di = di[len(granular_levels):]

# Now you can analyze granular_di and coarse_di to understand how DI changes.
# For example, you can calculate statistics like mean, standard deviation, min, max.

print("Analysis of DI with Finer Granularity:")
print(f"  Mean: {np.mean(granular_di):.4f}")
print(f"  Standard Deviation: {np.std(granular_di):.4f}")
print(f"  Minimum: {np.min(granular_di):.4f}")
print(f"  Maximum: {np.max(granular_di):.4f}")

print("\nAnalysis of DI with Coarser Granularity:")
print(f"  Mean: {np.mean(coarse_di):.4f}")
print(f"  Standard Deviation: {np.std(coarse_di):.4f}")
print(f"  Minimum: {np.min(coarse_di):.4f}")
print(f"  Maximum: {np.max(coarse_di):.4f}")

# You can also plot them to visually compare:
plt.figure(figsize=(10, 5))
plt.plot(granular_levels, granular_di, label="Finer Granularity")
plt.plot(coarse_levels, coarse_di, label="Coarser Granularity")
plt.xlabel("Repair Level")
plt.ylabel("Disparate Impact")
plt.title("Disparate Impact with Different Granularities")
plt.legend()
plt.grid(True)
plt.show()

# Show grid for readability
plt.grid(True)

# Display the plot
plt.show()

from IPython.display import display
display(plt.gcf())

"""Answer 2.1"""

import matplotlib.pyplot as plt
import numpy as np


# Separate granular and coarse levels and their corresponding DI values
granular_levels = np.linspace(0., 0.1, 81)
coarse_levels = np.linspace(0.2, 1, 20)

granular_di = di[:len(granular_levels)]
coarse_di = di[len(granular_levels):]

# Now you can analyze granular_di and coarse_di to understand how DI changes.
# For example, you can calculate statistics like mean, standard deviation, min, max.

print("Analysis of DI with Finer Granularity:")
print(f"  Mean: {np.mean(granular_di):.4f}")
print(f"  Standard Deviation: {np.std(granular_di):.4f}")
print(f"  Minimum: {np.min(granular_di):.4f}")
print(f"  Maximum: {np.max(granular_di):.4f}")

print("\nAnalysis of DI with Coarser Granularity:")
print(f"  Mean: {np.mean(coarse_di):.4f}")
print(f"  Standard Deviation: {np.std(coarse_di):.4f}")
print(f"  Minimum: {np.min(coarse_di):.4f}")
print(f"  Maximum: {np.max(coarse_di):.4f}")

"""How Does Disparate Impact (DI) Change with Granularity? (Defaulting Example)

Disparate Impact (DI) measures whether an AI model disproportionately favors or disadvantages certain groups. It is calculated as:

DI = Selection Rate of Unprivileged Group / Selection Rate of Privileged Group

A DI score closer to 1.0 indicates fairness, while a lower DI suggests bias.

Effect of Finer Granularity on DI:
Finer granularity breaks groups into smaller subcategories for a more detailed assessment.

Example: A bank categorizes borrowers into Male (10% default rate) and Female (15% default rate), resulting in DI = 0.67. If age is added as a factor:

Males 18-30: 12%, 31-50: 9%, 51+: 7%

Females 18-30: 18%, 31-50: 14%, 51+: 11%

Now, DI varies by age, revealing hidden biases. Younger females (DI = 0.67) face more discrimination than older females (DI = 0.78).

Effect of Coarser Granularity on DI:
Coarser granularity merges subgroups, making DI appear more stable but potentially hiding disparities.

Example: If all females are grouped together (15% default rate), age-based bias is masked, and DI remains 0.67 without showing variation.

Key Takeaways:

Finer Granularity → More precise DI but higher fluctuation.

Coarser Granularity → Smoother DI but may hide subgroup biases.

Balanced Approach → Detect bias while minimizing noise.


"""

import numpy as np

# Define performance metrics for each model
performance_metrics = {
    "Original": {
        "accuracy": 0.8153,
        "precision": 0.6400,
        "recall": 0.5454,
        "f1": 0.5889,
        "roc_auc": 0.8003,
    },
    "Reweighted": {
        "accuracy": 0.8111,
        "precision": 0.6275,
        "recall": 0.5437,
        "f1": 0.5826,
        "roc_auc": 0.7960,
    },
    "DIR": {
        "accuracy": 0.8103,
        "precision": 0.6258,
        "recall": 0.5415,
        "f1": 0.5806,
        "roc_auc": 0.7950,
    },
}

# Define fairness metrics for each model
fairness_metrics = {
    "Original": {
        "SPD": -0.0862,
        "DI": 0.8972,
        "EOD": 0.0164,
        "AOD": -0.0163,
    },
    "Reweighted": {
        "SPD": -0.0351,
        "DI": 0.9565,
        "EOD": 0.0164,
        "AOD": -0.0163,
    },
    "DIR": {
        "SPD": -0.1097,
        "DI": 0.8659,
        "EOD": -0.0226,
        "AOD": -0.0694,
    },
}

# Define an ideal Disparate Impact (DI) value (1.0 means perfect fairness)
ideal_DI = 1.0

# Function to calculate fairness score based on proximity to DI = 1
def fairness_score(model_fairness):
    di_difference = abs(model_fairness["DI"] - ideal_DI)
    return di_difference

# Calculate overall fairness score for each model
fairness_scores = {model: fairness_score(fairness) for model, fairness in fairness_metrics.items()}

# Print all scores
for model in performance_metrics.keys():
    print(f"\n--- {model} Model ---")
    print("Performance Metrics:")
    for metric, value in performance_metrics[model].items():
        print(f"  {metric}: {value:.4f}")

    print("Fairness Metrics:")
    for metric, value in fairness_metrics[model].items():
        print(f"  {metric}: {value:.4f}")

    print(f"  Fairness Score (DI deviation): {fairness_scores[model]:.4f}")

# Determine the best model (balancing performance and fairness)
best_model = min(fairness_scores, key=fairness_scores.get)

print(f"\nRecommended Model: {best_model}")

"""**2.2. Based on your findings which model would you recommend (Original, Reweighted, or DIR)? Justify your choice by balancing performance and fairness.**

**The Reweighted Model is the best recommendation because:
It achieves the best fairness score (DI closest to 1.0).
It still maintains good performance without a major drop in accuracy or precision.
It balances both fairness and model effectiveness better than the Original and DIR models.**

**2.3. Module Reflection Question: Consider a scenario where you are responsible for deploying a machine learning model that shows a fairness-accuracy trade-off. How would you approach deciding whether to prioritize fairness or accuracy, and which factors (i.e. ogranizational values, societal impact, or application context) would guide your decision? Reflect on how ethical dilemmas in machine learning require balancing competing values and their implications.**

2.3 Answer

The following reflection explores the ethical dilemmas in machine learning, emphasizing the trade-off between fairness and accuracy. Key considerations include organizational values, societal impact, and the specific application context.

Deploying machine learning models that strike a balance between fairness and accuracy requires a principled approach aligned with ethical standards, social responsibility, and business objectives. Organizations must determine their priorities: if efficiency and profitability are the focus, accuracy is paramount for precise predictions; if inclusivity and ethical responsibility take precedence, fairness must be prioritized to mitigate bias and discrimination. For instance, fairness is crucial in hiring models to foster diversity, whereas fraud detection systems prioritize accuracy to minimize financial risk.

Societal values such as justice and accessibility also influence this balance, as AI models can either reinforce or mitigate systemic inequalities. In high-stakes fields like healthcare, finance, or law enforcement, fairness is essential to prevent harm to marginalized groups. A healthcare model might prioritize accuracy for reliable diagnoses, while a loan approval system must carefully balance fairness and accuracy to promote inclusivity without exposing financial institutions to undue risk.

This trade-off presents ethical challenges: focusing too much on fairness may compromise accuracy (e.g., overlooking fraudulent activity), whereas neglecting fairness can perpetuate biases. Transparency and accountability—supported by frameworks like GDPR—help ensure AI decisions remain explainable and ethically sound.

Ultimately, responsible AI development requires ongoing efforts to optimize both fairness and accuracy, with continuous monitoring to adapt to evolving ethical, societal, and business considerations. A well-designed model should not only deliver strong performance but also uphold principles of equity, inclusivity, and justice.
"""

#End of Assignment